# Developer Checklist for Data Analyzer - ACTIVE TASKS ONLY
# Status: TODO | INPROGRESS | DONE
# Completed tasks moved to: archive/developer_checklist_archived.yaml
# AI agents: Please update status and add notes when working on tasks

project_info:
  name: Data Analyzer - Active Development
  version: 1.0.0
  last_updated: 2025-09-28T20:00:00Z

# =========================================================
# CRITICAL UPDATE - 2025-09-28 - READ THIS FIRST
# =========================================================
# FIXES COMPLETED TODAY BY CLAUDE:
# 1. ✅ Export dropdown - Shows download button immediately
# 2. ✅ PDF dictionary caching - Saves to ~/.data_analyzer_cache/
# 3. ✅ PDF progress - Shows every page update (not batched)
# 4. ✅ Port issue - Fixed multiple instances, runs on 8501
# 5. ⚠️  Mermaid diagram - STILL BROKEN (see core_1c below)
#
# MAJOR DISCOVERIES - IMPORTANT:
# - MCPClient is 100% SIMULATED - NO real MCP (see core_1)
# - Azure OpenAI was NEVER IMPLEMENTED (see core_1b)
# - Dictionary parsing uses BASIC REGEX ONLY
# - The app has been running on simulations all along!
# =========================================================

# PRIORITY 1: Frontend Cleanup & Visual Improvements
phase_frontend_cleanup:
  name: "Frontend Visual Overhaul"
  status: TODO
  priority: CRITICAL
  tasks:
    - id: ui_cleanup_1
      name: "Fix Navigation Bar"
      description: "Professional navbar with better styling and responsiveness"
      status: DONE
      priority: CRITICAL
      effort: "4 hours"
      completed_date: "2025-09-28T07:45:00Z"
      notes: "Implemented horizontal navigation with streamlit-option-menu, custom gradient styling"
      subtasks:
        - "Fix navbar spacing and alignment"
        - "Add hover effects and transitions"
        - "Ensure mobile responsiveness"
        - "Fix active page highlighting"
        - "Add logo/branding area"
        - "Implement breadcrumb navigation"

    - id: ui_cleanup_2
      name: "Sidebar Reorganization"
      description: "Streamline sidebar to reduce clutter"
      status: TODO
      priority: CRITICAL
      effort: "3 hours"
      subtasks:
        - "Group related controls in expanders"
        - "Move debug/advanced options to collapsible section"
        - "Add icons to all controls"
        - "Implement smart defaults"
        - "Add tooltips for all options"
        - "Create 'Quick Start' section at top"

    - id: ui_cleanup_3
      name: "Main Content Area Restructuring"
      description: "Reduce scrolling and improve information hierarchy"
      status: DONE
      priority: CRITICAL
      effort: "6 hours"
      completed_date: "2025-09-28T07:45:00Z"
      notes: "Implemented tabbed interface for Analysis page (Upload, Analyze, Results, Export tabs)"
      subtasks:
        - "Replace vertical layout with tabbed interface"
        - "Create summary dashboard as first view"
        - "Move detailed results to secondary tabs"
        - "Implement lazy loading for heavy content"
        - "Add 'Jump to Section' navigation"
        - "Create collapsible result cards"

    - id: ui_cleanup_4
      name: "Schema/Rules Editor Overhaul"
      description: "Replace individual inputs with table-based editor"
      status: TODO
      priority: HIGH
      effort: "5 hours"
      dependencies: []
      subtasks:
        - "Install streamlit-aggrid for Excel-like editing"
        - "Create bulk edit interface"
        - "Add copy/paste support"
        - "Implement undo/redo functionality"
        - "Add import from clipboard feature"
        - "Create schema templates dropdown"

    - id: ui_cleanup_5
      name: "Results Dashboard Redesign"
      description: "Professional analytics dashboard"
      status: TODO
      priority: HIGH
      effort: "6 hours"
      subtasks:
        - "Create status cards with metrics"
        - "Add progress indicators during analysis"
        - "Implement color-coded severity levels"
        - "Create interactive issue explorer"
        - "Add filtering and sorting for issues"
        - "Implement issue grouping by type/column"

    - id: ui_cleanup_6
      name: "Visual Polish & Theming"
      description: "Professional appearance with consistent design"
      status: DONE
      priority: MEDIUM
      effort: "4 hours"
      completed_date: "2025-09-28T07:45:00Z"
      notes: "Added gradient backgrounds, professional cards, hover effects, modern button styling"
      subtasks:
        - "Implement consistent color scheme"
        - "Add Tailwind CSS or Bootstrap integration"
        - "Create custom CSS for components"
        - "Add smooth animations and transitions"
        - "Implement dark mode toggle"
        - "Fix spacing and padding throughout"

    - id: ui_cleanup_7
      name: "Responsive Design"
      description: "Ensure works well on all screen sizes"
      status: DONE
      priority: MEDIUM
      effort: "3 hours"
      completed_date: "2025-09-28T07:48:00Z"
      notes: "Added comprehensive media queries for mobile, tablet, and desktop views"
      subtasks:
        - "Test on mobile devices"
        - "Implement responsive grid layouts"
        - "Create mobile-friendly navigation"
        - "Optimize for tablet views"
        - "Add viewport meta tags"

    - id: ui_cleanup_8
      name: "Loading States & Feedback"
      description: "Better user feedback during operations"
      status: DONE
      priority: HIGH
      effort: "2 hours"
      completed_date: "2025-09-28T07:45:00Z"
      notes: "Implemented progress bars, spinners, and status messages during analysis"
      subtasks:
        - "Add skeleton loaders"
        - "Implement progress bars for long operations"
        - "Add success/error toast notifications"
        - "Create loading overlays"
        - "Add operation time estimates"

# PRIORITY 2: Download with Error Highlighting Feature
phase_error_export:
  name: "Export Data with Error Highlighting"
  status: TODO
  priority: HIGH
  tasks:
    - id: export_1
      name: "Excel Export with Error Highlighting"
      description: "Download Excel files with errors highlighted in red"
      status: TODO
      priority: HIGH
      effort: "1 day"
      subtasks:
        - "Use openpyxl for Excel manipulation"
        - "Apply red background to error cells"
        - "Add error comments to cells"
        - "Create error summary sheet"
        - "Include validation report as separate sheet"
        - "Add conditional formatting for warnings"

    - id: export_2
      name: "CSV Export with Error Markers"
      description: "Mark errors in CSV format"
      status: TODO
      priority: HIGH
      effort: "4 hours"
      subtasks:
        - "Add _ERROR suffix to problematic values"
        - "Create companion error report file"
        - "Add error line numbers in separate column"
        - "Generate error mapping file"

    - id: export_3
      name: "JSON Export with Error Annotations"
      description: "Annotate JSON with error metadata"
      status: TODO
      priority: MEDIUM
      effort: "4 hours"
      subtasks:
        - "Add _errors field to objects with issues"
        - "Include error types and messages"
        - "Maintain original structure"
        - "Create error summary at root level"

    - id: export_4
      name: "HTML Report Generation"
      description: "Interactive HTML report with errors"
      status: TODO
      priority: MEDIUM
      effort: "6 hours"
      subtasks:
        - "Create HTML template with styling"
        - "Highlight errors with CSS"
        - "Add interactive filtering"
        - "Include charts and visualizations"
        - "Make report self-contained (inline CSS/JS)"

    - id: export_5
      name: "Download UI Integration"
      description: "Add download buttons to interface"
      status: TODO
      priority: HIGH
      effort: "2 hours"
      subtasks:
        - "Add download section to results"
        - "Provide format selection dropdown"
        - "Show download size estimate"
        - "Add batch download for all formats"

# PRIORITY 3: Testing & Debugging Core Functionality
phase_testing_debugging:
  name: "Fix Core Bugs and Test All Features"
  status: INPROGRESS
  priority: CRITICAL
  tasks:
    - id: test_debug_1
      name: "Fix JSON Format Support"
      description: "JSON analysis currently broken - showing 'Unsupported format: json'"
      status: TODO
      priority: CRITICAL
      effort: "2 hours"
      bug_reported: "User reported: JSON Mixed demo data causes error"
      subtasks:
        - "Debug _simulate_mcp_call method for JSON handling"
        - "Fix JSON data loading in web_app.py"
        - "Test with all JSON demo formats"
        - "Handle nested JSON structures properly"
        - "Fix JSON preview display"

    - id: test_debug_2
      name: "Test All Demo Data Formats"
      description: "Systematically test each demo data format"
      status: TODO
      priority: CRITICAL
      effort: "3 hours"
      subtasks:
        - "Test CSV - Western demo data"
        - "Test CSV - Asian demo data"
        - "Test JSON - Mixed demo data"
        - "Test CSV - Clinical Trial demo data"
        - "Document all errors found"
        - "Fix each format's specific issues"

    - id: test_debug_3
      name: "Data Dictionary Testing"
      description: "Test data dictionary parsing and application"
      status: TODO
      priority: HIGH
      effort: "2 hours"
      subtasks:
        - "Test loading each demo dictionary"
        - "Verify schema extraction works"
        - "Test rules extraction"
        - "Apply dictionaries to matching demo data"
        - "Fix any parsing errors"
        - "Test dictionary UI integration"

    - id: test_debug_4
      name: "File Upload Testing"
      description: "Test various file upload scenarios"
      status: TODO
      priority: HIGH
      effort: "2 hours"
      subtasks:
        - "Test CSV file uploads"
        - "Test Excel file uploads"
        - "Test JSON file uploads"
        - "Test Parquet file uploads"
        - "Test large file handling"
        - "Test invalid file formats"
        - "Test empty files"

    - id: test_debug_5
      name: "Schema and Rules Validation Testing"
      description: "Verify schema and rules work correctly"
      status: TODO
      priority: HIGH
      effort: "2 hours"
      subtasks:
        - "Test manual schema creation"
        - "Test auto-detection of types"
        - "Test rules with min/max values"
        - "Test allowed values validation"
        - "Test datetime validation"
        - "Fix validation error messages"

    - id: test_debug_6
      name: "Error Display Testing"
      description: "Ensure errors are displayed correctly"
      status: TODO
      priority: HIGH
      effort: "1 hour"
      subtasks:
        - "Test error highlighting in results"
        - "Verify error counts are accurate"
        - "Test error message clarity"
        - "Check error navigation"
        - "Test error export functionality"

    - id: test_debug_7
      name: "Cross-Browser Testing"
      description: "Test application in different browsers"
      status: TODO
      priority: MEDIUM
      effort: "1 hour"
      subtasks:
        - "Test in Chrome"
        - "Test in Firefox"
        - "Test in Safari"
        - "Test in Edge"
        - "Document browser-specific issues"

    - id: test_debug_8
      name: "Performance Testing"
      description: "Test with various data sizes"
      status: TODO
      priority: MEDIUM
      effort: "2 hours"
      subtasks:
        - "Test with 100 rows"
        - "Test with 1,000 rows"
        - "Test with 10,000 rows"
        - "Test with 100,000 rows"
        - "Monitor memory usage"
        - "Check for memory leaks"

# PRIORITY 4: Performance & UX Improvements
phase_performance_ux:
  name: "Performance and User Experience"
  status: TODO
  tasks:
    - id: perf_ux_1
      name: "Implement Caching"
      description: "Cache analysis results and schemas"
      status: TODO
      priority: MEDIUM
      effort: "4 hours"
      subtasks:
        - "Use st.cache_data for analysis results"
        - "Cache uploaded file processing"
        - "Store user preferences in session"
        - "Implement smart cache invalidation"

    - id: perf_ux_2
      name: "Streaming for Large Files"
      description: "Handle files > 100MB efficiently"
      status: TODO
      priority: HIGH
      effort: "1 day"
      subtasks:
        - "Implement chunked file reading"
        - "Add streaming preview"
        - "Progressive analysis with updates"
        - "Memory usage monitoring"

    - id: perf_ux_3
      name: "Keyboard Shortcuts"
      description: "Power user features"
      status: TODO
      priority: LOW
      effort: "2 hours"
      subtasks:
        - "Ctrl+Enter to run analysis"
        - "Tab navigation between sections"
        - "Escape to close modals"
        - "Ctrl+S to save configuration"

    - id: perf_ux_4
      name: "Configuration Save/Load"
      description: "Save and reuse configurations"
      status: TODO
      priority: MEDIUM
      effort: "3 hours"
      subtasks:
        - "Export schema/rules as JSON"
        - "Import saved configurations"
        - "Create configuration library"
        - "Share configurations via URL"

# PRIORITY 5A: LLM Integration (IMMEDIATE)
phase_llm_integration:
  name: "LLM/Azure OpenAI Implementation"
  status: DONE
  description: "Get actual AI working with API keys in .env"
  completed: "2025-09-28"
  tasks:
    - id: llm_1
      name: "Basic Azure OpenAI Connection"
      description: "Connect to Azure OpenAI using existing .env credentials"
      status: DONE
      priority: CRITICAL
      effort: "2-3 hours"
      notes: |
        # COMPLETED:
        # ✅ Created src/llm_client.py with Azure OpenAI integration
        # ✅ Successfully connected to Azure OpenAI endpoint
        # ✅ Using gpt-4o-mini model for cost-effective processing
        # ✅ Implemented token counting and chunking for large documents
        # 4. Create connection class
        # 5. Test with simple prompt

        # Required .env variables:
        # AZURE_OPENAI_API_KEY=xxx
        # AZURE_OPENAI_ENDPOINT=xxx
        # AZURE_OPENAI_DEPLOYMENT_NAME=xxx

    - id: llm_2
      name: "Dictionary Parser with LLM"
      description: "Replace regex parsing with GPT-4 extraction"
      status: DONE
      priority: HIGH
      effort: "3-4 days"
      notes: |
        # COMPLETED:
        # ✅ Implemented LLMDictionaryParser class in src/llm_client.py
        # ✅ Added support for CSV, PDF, JSON, TXT dictionaries
        # ✅ Integrated into web app with checkbox toggle
        # ✅ Extracts field names, types, ranges, allowed values
        # ✅ Successfully tested with sample dictionaries
        # ✅ Returns structured schema for validation
        #    - Skip patterns (basic, not full logic)
        # 3. Return structured JSON schema
        # 4. Cache results to avoid API costs

    - id: llm_3
      name: "Enhanced Data Quality Checks"
      description: "Use LLM to detect complex data issues"
      status: DONE
      priority: MEDIUM
      effort: "2-3 days"
      notes: |
        # COMPLETED:
        # ✅ Implemented validate_data_with_llm() method
        # ✅ Detects semantic inconsistencies
        # ✅ Identifies data entry errors
        # ✅ Finds range violations and missing required fields
        # ✅ Provides detailed issue reports with severity levels
        # ✅ Generates actionable recommendations
        # ✅ Successfully tested with sample data

# PRIORITY 5B: Logic Validation (COMPLEX)
phase_logic_validation:
  name: "Conditional Logic Engine"
  status: TODO
  description: "The hardest problem - validating conditional rules"
  tasks:
    - id: logic_1
      name: "Template-Based Logic Detection"
      description: "Start with common patterns"
      status: TODO
      priority: HIGH
      effort: "3-4 days"
      notes: |
        # Phase 1 - Simple templates
        # Detect patterns like:
        # - "If X = Y, then Z is required"
        # - "Skip to Q if condition"
        # - "Show only if"

        # Create src/logic_templates.py

    - id: logic_2
      name: "Statistical Logic Discovery"
      description: "Learn patterns from data"
      status: TODO
      priority: MEDIUM
      effort: "5-7 days"
      notes: |
        # Analyze actual data to find patterns
        # When field A=X, field B is always blank
        # Build confidence scores
        # Flag violations

    - id: logic_3
      name: "LLM Logic Extraction"
      description: "Use GPT-4 to understand complex logic"
      status: TODO
      priority: HIGH
      effort: "1 week"
      notes: |
        # Send dictionary sections to GPT-4
        # Extract complex conditional rules
        # Generate Python validation code
        # This is part of core_1b2 task

# PRIORITY 5C: Core Infrastructure
phase_core_remaining:
  name: "Core Infrastructure Improvements"
  status: TODO
  tasks:
    - id: core_1
      name: "MCP Client Integration"
      description: "Replace simulated MCP calls with actual client"
      status: TODO
      priority: MEDIUM
      effort: "3-5 days"
      notes: |
        CRITICAL CONTEXT - MUST READ:
        # Current State (as of 2025-09-28):
        # - The ENTIRE MCPClient class in web_app.py (line 181) is SIMULATED
        # - NO real MCP server connection exists
        # - analyze_data_quality() method uses basic pattern matching, NOT AI
        # - This is NOT a bug - it was intentionally left as simulation

        # What's Being Simulated:
        # 1. Data quality analysis - just regex checks for "invalid", "error", etc.
        # 2. Range validation - simple numeric comparisons
        # 3. Missing value detection - basic pandas isnull() checks
        # 4. NO machine learning or AI analysis happening

        # To Implement Real MCP:
        # - Install actual MCP client library
        # - Replace MCPClient class with real implementation
        # - Connect to actual MCP server (mcp_server.py exists but not connected)
        # - Remove all simulation code

    - id: core_1b
      name: "Azure OpenAI/LLM Dictionary Parsing"
      description: "Implement AI-powered dictionary parsing"
      status: TODO
      priority: HIGH
      effort: "5-7 days"
      notes: |
        IMPORTANT: This feature was NEVER IMPLEMENTED despite being in docs
        # Current State (as of 2025-09-28):
        # - NO Azure OpenAI code exists in repository (verified with grep)
        # - NO LLM integration exists
        # - PDF dictionary parsing uses BASIC REGEX ONLY:
        #   * Date fields: looks for "date" in field names
        #   * Ranges: looks for "range/between" + two numbers
        #   * That's it! Very primitive.

        # Original Plan (never built):
        # - Use Azure OpenAI GPT-4o-mini to generate Python parser code
        # - Parse complex dictionaries (REDCap, Clinical, etc.)
        # - Generate deterministic, verifiable parser code
        # - Cache parsers for reuse

        # Why It Matters:
        # - Current regex parsing is almost useless for real dictionaries
        # - Can't detect: data types, enumerations, patterns, dependencies
        # - Can't understand complex formats or nested structures

        # To Implement:
        # 1. Add openai Python library to requirements
        # 2. Add Azure OpenAI credentials to .env
        # 3. Create LLM prompt for dictionary parsing
        # 4. Generate Python parser code via LLM
        # 5. Execute parser in sandbox with timeout
        # 6. Cache successful parsers

    - id: core_1b2
      name: "Conditional Logic Validation Engine"
      description: "Detect and validate conditional logic rules in data"
      status: TODO
      priority: CRITICAL
      effort: "10-15 days"
      notes: |
        MAJOR FEATURE: This is one of the hardest problems in data quality
        # The Problem:
        # - Data dictionaries contain conditional logic (skip patterns, branching)
        # - Example: "If Question 3 = 'No', skip to Question 7"
        # - Example: "If Age < 18, ParentConsent must be 'Yes'"
        # - Example: "If Treatment = 'Placebo', DoseAmount should be blank"
        # - EVERY dictionary expresses this differently!

        # Why It's Hard:
        # 1. No standard format for expressing logic
        # 2. Logic can be in text, tables, or separate documents
        # 3. Complex nested conditions (IF A=1 AND B=2 THEN C must be blank)
        # 4. Temporal dependencies (If Visit1_Status = 'Complete', Visit2 can be entered)

        # APPROACH 1: LLM-Powered Rule Extraction
        # =========================================
        # Use GPT-4 to extract logical rules and generate validation code
        #
        # Pros:
        # - Can understand natural language descriptions
        # - Handles various formats (REDCap, Clinical, custom)
        # - Can generate Python validation functions
        #
        # Cons:
        # - Expensive (API calls)
        # - Not 100% accurate
        # - Need to verify generated code
        #
        # Implementation:
        # 1. Send dictionary pages to GPT-4 with specialized prompt
        # 2. Extract rules in structured format (JSON)
        # 3. Generate Python validation functions
        # 4. Test on sample data
        # 5. Cache successful validators

        # APPROACH 2: Statistical Pattern Learning
        # =========================================
        # Learn patterns from the data itself (unsupervised)
        #
        # Pros:
        # - No need to parse dictionary
        # - Finds actual patterns in real data
        # - Can detect undocumented rules
        #
        # Cons:
        # - Needs sufficient data
        # - May find spurious correlations
        # - Can't distinguish required vs coincidental patterns
        #
        # Implementation:
        # 1. Analyze field combinations in actual data
        # 2. Find patterns: "When A=X, B is always Y or blank"
        # 3. Calculate confidence scores (95% = likely rule)
        # 4. Flag violations of high-confidence patterns
        # 5. Present findings for human review

        # APPROACH 3: Hybrid Template + Learning
        # =========================================
        # Combine template matching with ML pattern discovery
        #
        # Pros:
        # - Best of both worlds
        # - Can handle known and unknown patterns
        # - More robust than either alone
        #
        # Cons:
        # - Most complex to implement
        # - Requires both dictionary AND data
        # - Needs careful tuning
        #
        # Implementation:
        # 1. Pre-built templates for common patterns:
        #    - "If [field] = [value], then [field2] = [value2/blank]"
        #    - "Skip to [field] if [condition]"
        #    - "Required if [condition]"
        # 2. Use regex/NLP to match templates in dictionary
        # 3. Validate patterns against actual data
        # 4. Use ML to find additional patterns not in templates
        # 5. Combine findings with confidence scores

        # Example Logic Patterns to Detect:
        # - Skip logic: "Skip Q5-Q10 if Q4='No'"
        # - Conditional requirements: "If pregnant='Yes', due_date required"
        # - Mutual exclusivity: "If option_A selected, option_B must be blank"
        # - Range dependencies: "If age<18, parent_signature required"
        # - Cascade logic: "If country='US', show state dropdown"

        # Recommended Starting Point:
        # Start with Approach 3 (Hybrid) because:
        # - Can begin with simple template matching
        # - Add LLM enhancement later
        # - Statistical validation provides safety net
        # - Most practical for production use

    - id: core_1c
      name: "Fix Mermaid Diagram Rendering"
      description: "Mermaid diagram shows as text instead of flowchart"
      status: TODO
      priority: LOW
      effort: "2-3 hours"
      notes: |
        # Issue (as of 2025-09-28):
        # - Mermaid diagram in About page shows raw text not visual
        # - Located in assets/data_flow_diagram.mmd
        # - render_mermaid() function creates iframe but Mermaid.js fails
        # - Error: "transform: Expected number, translate(undefined, NaN)"

        # Attempted Fixes That Failed:
        # 1. Simplified diagram syntax (graph LR)
        # 2. Installed streamlit-mermaid package
        # 3. Used st_mermaid() function
        # 4. All result in empty iframe or text display

        # Possible Solutions:
        # - Use graphviz instead of Mermaid
        # - Use static PNG/SVG image instead
        # - Try different Mermaid.js version
        # - Use Streamlit's native diagram support if available

    - id: core_2
      name: "Error Recovery"
      description: "Graceful error handling throughout"
      status: TODO
      priority: HIGH
      effort: "1 day"
      subtasks:
        - "Add try-catch blocks everywhere"
        - "Provide helpful error messages"
        - "Add retry mechanisms"
        - "Log errors for debugging"

# PRIORITY 6: Testing & Documentation
phase_testing_docs:
  name: "Testing and Documentation"
  status: TODO
  tasks:
    - id: test_1
      name: "UI Testing"
      description: "Test all UI components"
      status: TODO
      priority: MEDIUM
      effort: "1 day"
      subtasks:
        - "Test file upload scenarios"
        - "Verify all buttons work"
        - "Check responsive design"
        - "Test error states"

    - id: doc_1
      name: "User Guide"
      description: "Comprehensive user documentation"
      status: TODO
      priority: HIGH
      effort: "4 hours"
      subtasks:
        - "Getting started tutorial"
        - "Feature walkthroughs"
        - "Video demonstrations"
        - "FAQ section"

# Known Bugs to Fix
bugs:
  - id: bug_1
    description: "Large files (>200MB) cause memory issues"
    status: TODO
    priority: HIGH

  - id: bug_2
    description: "Special characters in CSV headers cause issues"
    status: TODO
    priority: MEDIUM

  - id: bug_3
    description: "Navbar doesn't highlight active page correctly"
    status: TODO
    priority: HIGH

  - id: bug_4
    description: "Schema editor allows duplicate column names"
    status: TODO
    priority: MEDIUM

  - id: bug_5
    description: "File upload doesn't clear previous analysis"
    status: TODO
    priority: HIGH

# Immediate Action Items (For Tonight's Work)
immediate_tasks:
  order_of_execution:
    1: "test_debug_1 - Fix JSON Format Support (CRITICAL BUG)"
    2: "test_debug_2 - Test All Demo Data Formats"
    3: "test_debug_3 - Data Dictionary Testing"
    4: "ui_cleanup_1 - Fix Navigation Bar"
    5: "ui_cleanup_2 - Sidebar Reorganization"
    6: "ui_cleanup_3 - Main Content Area Restructuring"
    7: "ui_cleanup_6 - Visual Polish (CSS/theming)"
    8: "ui_cleanup_5 - Results Dashboard Redesign"
    9: "export_1 - Excel Export with Error Highlighting"
    10: "export_5 - Download UI Integration"

  estimated_time: "10-12 hours total"

  notes: |
    CRITICAL: Fix JSON support bug first - user reported this is broken!
    Then ensure all core functionality works before UI improvements.
    Focus on making the UI clean, professional, and less cluttered.
    Priority is reducing scrolling and improving visual hierarchy.
    The download feature with error highlighting is a key differentiator.

# AI Agent Instructions
ai_instructions:
  approach: |
    1. Start with ui_cleanup_1 through ui_cleanup_3 for maximum impact
    2. Use streamlit native components where possible
    3. Add streamlit-aggrid only for schema editor
    4. Test each change before moving to next task
    5. Commit after each major task completion
    6. Focus on user experience over perfect code

  style_guide: |
    - Use consistent spacing (st.markdown("&nbsp;") for gaps)
    - Apply color coding: green=success, yellow=warning, red=error
    - Use icons from emoji set consistently
    - Keep sidebar width reasonable (don't overcrowd)
    - Ensure all text is readable (good contrast)

  testing: |
    - Test with demo data after each change
    - Verify no functionality is broken
    - Check responsive design at different widths
    - Ensure loading states work properly